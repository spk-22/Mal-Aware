{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uLEbZGy16uL",
        "outputId": "9b9812a8-accd-4b85-d6dd-c8636af15f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install torch torch-geometric scikit-learn pandas networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGlEOUz6_kjZ",
        "outputId": "4e3b1c85-d2cc-4f01-d999-aa002643b729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=7ca841951858fb1df5e592c05ccd68fb39896bafc0e2ce341888fd4723d5daa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=1127937 sha256=598ce6300e795e9e617bc01b7193e098e36a926d1c2ab559e78a68749e7d368f\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "Successfully built torch-scatter torch-sparse\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "Successfully installed torch-scatter-2.1.2 torch-sparse-0.6.18\n"
          ]
        }
      ],
      "source": [
        "pip install torch-geometric torch-scatter torch-sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRSBddlF81Wc",
        "outputId": "d6818571-88bb-47ba-944d-c41bc8dfcedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Malware/Output1.xlsx\")\n",
        "\n",
        "# Extract features and labels\n",
        "features = df.iloc[:, 1:].values\n",
        "filenames = df['Filename']\n",
        "\n",
        "# Encode labels (e.g., 'Spyware-TIBS' -> 'Spyware')\n",
        "labels = filenames.str.extract(r'(\\w+)-')[0]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Create edges based on similarity\n",
        "sim_matrix = cosine_similarity(features)\n",
        "k = 5  # k nearest neighbors\n",
        "\n",
        "# Creating time windows of 10 rows each (85 windows for 851 rows)\n",
        "window_size = 10\n",
        "num_windows = len(df) // window_size  # Total windows = 851 / 10 = 85\n",
        "\n",
        "# Create a writer object to write data to a single Excel file\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/Malware/all_windows_data.xlsx') as writer:\n",
        "    # Store data for each time window in a list\n",
        "    for window in range(num_windows):\n",
        "        start_idx = window * window_size\n",
        "        end_idx = (window + 1) * window_size\n",
        "\n",
        "        # Extract features for the current window\n",
        "        window_features = features[start_idx:end_idx]\n",
        "        window_labels = y[start_idx:end_idx]\n",
        "\n",
        "        # Create a similarity matrix for this window\n",
        "        sim_matrix_window = cosine_similarity(window_features)\n",
        "\n",
        "        # Generate edges for this window based on similarity\n",
        "        edge_index_window = []\n",
        "        for i in range(len(sim_matrix_window)):\n",
        "            top_k = np.argsort(sim_matrix_window[i])[-(k+1):-1]  # Skip self-loop\n",
        "            for j in top_k:\n",
        "                edge_index_window.append([i, j])\n",
        "\n",
        "        # Convert to tensor\n",
        "        edge_index_window = torch.tensor(edge_index_window, dtype=torch.long).t().contiguous()\n",
        "        x_window = torch.tensor(window_features, dtype=torch.float)\n",
        "        y_window = torch.tensor(window_labels, dtype=torch.long)\n",
        "\n",
        "        # Create PyG Data object for this window\n",
        "        data_window = Data(x=x_window, edge_index=edge_index_window, y=y_window)\n",
        "\n",
        "        # Convert x (features) to DataFrame and save to Excel (one sheet per window)\n",
        "        df_x = pd.DataFrame(x_window.numpy())  # Convert tensor to numpy and then to DataFrame\n",
        "        df_x.to_excel(writer, sheet_name=f'Window_{window}_Features', index=False)\n",
        "\n",
        "        # Convert edge_index (edges) to DataFrame (source, target) and save to Excel\n",
        "        df_edges = pd.DataFrame(edge_index_window.numpy().T, columns=[\"Source\", \"Target\"])\n",
        "        df_edges.to_excel(writer, sheet_name=f'Window_{window}_Edges', index=False)\n",
        "\n",
        "        # Convert y (labels) to DataFrame and save to Excel\n",
        "        df_y = pd.DataFrame(y_window.numpy(), columns=[\"Labels\"])\n",
        "        df_y.to_excel(writer, sheet_name=f'Window_{window}_Labels', index=False)\n",
        "\n",
        "# The Excel file will now contain multiple sheets: one for each window's features, edges, and labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isOGeEaO_oth"
      },
      "source": [
        " Step 1: Dynamic Graph Learning with GraphSAGE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Sample GraphSAGE Model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NL0nVO9BJ4b6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train over multiple windows"
      ],
      "metadata": {
        "id": "U1gyHWOKJ7RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch_geometric.nn import GraphSAGE  # Make sure GraphSAGE is imported\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Set model parameters\n",
        "in_channels = data_list[0].num_node_features\n",
        "hidden_channels = 32\n",
        "out_channels = len(set(y))  # number of unique labels\n",
        "\n",
        "# Define the model\n",
        "model = GraphSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train model on each window\n",
        "model.train()\n",
        "for epoch in range(20):  # number of training epochs\n",
        "    total_loss = 0\n",
        "    for data in data_list:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7vToJ5zJ9Tq",
        "outputId": "87dc2f3c-f836-426a-faa4-178984de653d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 215.1897\n",
            "Epoch 2, Loss: 397.1544\n",
            "Epoch 3, Loss: 290.2010\n",
            "Epoch 4, Loss: 195.0410\n",
            "Epoch 5, Loss: 124.2890\n",
            "Epoch 6, Loss: 75.4146\n",
            "Epoch 7, Loss: 47.2189\n",
            "Epoch 8, Loss: 34.1043\n",
            "Epoch 9, Loss: 15.3015\n",
            "Epoch 10, Loss: 3.6751\n",
            "Epoch 11, Loss: 1.8512\n",
            "Epoch 12, Loss: 1.5489\n",
            "Epoch 13, Loss: 1.3789\n",
            "Epoch 14, Loss: 1.2416\n",
            "Epoch 15, Loss: 1.1266\n",
            "Epoch 16, Loss: 1.0279\n",
            "Epoch 17, Loss: 0.9422\n",
            "Epoch 18, Loss: 0.8668\n",
            "Epoch 19, Loss: 0.8000\n",
            "Epoch 20, Loss: 0.7404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GraphSAGE is imported:"
      ],
      "metadata": {
        "id": "ZJ_oG4AhKo5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GraphSAGE"
      ],
      "metadata": {
        "id": "6OwqZFyqKrwS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Accuracy"
      ],
      "metadata": {
        "id": "xV18NmAMK4S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on all windows using CPU\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in data_list:\n",
        "        data = data.to(\"cpu\")  # Set device to CPU\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.num_nodes\n",
        "\n",
        "print(f\"Accuracy: {correct / total * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMaWeysvK8rv",
        "outputId": "120a84f9-4925-45e6-c8a2-5a9b2dc4bf1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causal Sampling (Temporal Neighbor Selection)"
      ],
      "metadata": {
        "id": "Aq7tZrqPLFfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated Training Loop with Causal Sampling:"
      ],
      "metadata": {
        "id": "ayydhP4ULuof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GraphSAGE\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Re-initialize the model for causal training\n",
        "in_channels = data_list[0].num_node_features\n",
        "hidden_channels = 32\n",
        "out_channels = len(set(y))\n",
        "model = GraphSAGE(in_channels, hidden_channels, out_channels).to(\"cpu\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Causal training: Only train on current and previous windows\n",
        "model.train()\n",
        "for epoch in range(20):  # Number of epochs\n",
        "    total_loss = 0\n",
        "    for t in range(1, len(data_list)):  # Start from t=1 to allow causal (t-1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Simulate causal memory: train on window t-1\n",
        "        past_data = data_list[t - 1].to(\"cpu\")\n",
        "        current_data = data_list[t].to(\"cpu\")\n",
        "\n",
        "        # Forward on past data\n",
        "        out_past = model(past_data.x, past_data.edge_index)\n",
        "        loss_past = loss_fn(out_past, past_data.y)\n",
        "\n",
        "        # Forward on current data\n",
        "        out_current = model(current_data.x, current_data.edge_index)\n",
        "        loss_current = loss_fn(out_current, current_data.y)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = loss_past + loss_current\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Causal Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpG6ikzcLn1r",
        "outputId": "a8cf7685-464b-44dd-9a50-d799cc4b9a2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Causal Loss: 412.9249\n",
            "Epoch 2, Causal Loss: 815.9091\n",
            "Epoch 3, Causal Loss: 598.9709\n",
            "Epoch 4, Causal Loss: 407.2735\n",
            "Epoch 5, Causal Loss: 260.7712\n",
            "Epoch 6, Causal Loss: 163.1069\n",
            "Epoch 7, Causal Loss: 97.0076\n",
            "Epoch 8, Causal Loss: 66.9272\n",
            "Epoch 9, Causal Loss: 31.4280\n",
            "Epoch 10, Causal Loss: 7.5416\n",
            "Epoch 11, Causal Loss: 3.5096\n",
            "Epoch 12, Causal Loss: 2.8721\n",
            "Epoch 13, Causal Loss: 2.5626\n",
            "Epoch 14, Causal Loss: 2.3151\n",
            "Epoch 15, Causal Loss: 2.1079\n",
            "Epoch 16, Causal Loss: 1.9299\n",
            "Epoch 17, Causal Loss: 1.7746\n",
            "Epoch 18, Causal Loss: 1.6376\n",
            "Epoch 19, Causal Loss: 1.5156\n",
            "Epoch 20, Causal Loss: 1.4063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation After Causal Training:"
      ],
      "metadata": {
        "id": "oNPp5bwGL6D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate only on the last window (simulate unseen future)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    eval_data = data_list[-1].to(\"cpu\")  # Last window\n",
        "    out = model(eval_data.x, eval_data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    correct += (pred == eval_data.y).sum().item()\n",
        "    total += eval_data.num_nodes\n",
        "\n",
        "print(f\"Causal Evaluation Accuracy (on last window): {correct / total * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPb-qFJLL6qz",
        "outputId": "7d6884a0-2252-4eda-98bb-6688e0415e2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Causal Evaluation Accuracy (on last window): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inject 10–20% Noise for Robustness Testing"
      ],
      "metadata": {
        "id": "SVJbsO5NMY4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll inject:\n",
        "\n",
        "Label noise: randomly flip labels for 10–20% of nodes.\n",
        "\n",
        "Feature noise: add small Gaussian noise to features for 10–20% of nodes."
      ],
      "metadata": {
        "id": "Y8vxrtTIMcN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def inject_noise(data_list, label_noise_ratio=0.1, feature_noise_ratio=0.1):\n",
        "    noisy_data_list = []\n",
        "\n",
        "    for data in data_list:\n",
        "        data_noisy = copy.deepcopy(data)\n",
        "\n",
        "        # Inject label noise\n",
        "        num_nodes = data_noisy.y.shape[0]\n",
        "        num_label_noise = int(label_noise_ratio * num_nodes)\n",
        "        noisy_label_indices = random.sample(range(num_nodes), num_label_noise)\n",
        "\n",
        "        for idx in noisy_label_indices:\n",
        "            original_label = data_noisy.y[idx].item()\n",
        "            possible_labels = list(set(data_noisy.y.tolist()))\n",
        "            if len(possible_labels) <= 1:\n",
        "                continue\n",
        "            possible_labels.remove(original_label)\n",
        "            if not possible_labels:\n",
        "                continue\n",
        "            new_label = random.choice(possible_labels)\n",
        "            data_noisy.y[idx] = new_label\n",
        "\n",
        "        # Inject feature noise\n",
        "        num_feature_noise = int(feature_noise_ratio * num_nodes)\n",
        "        noisy_feature_indices = random.sample(range(num_nodes), num_feature_noise)\n",
        "\n",
        "        for idx in noisy_feature_indices:\n",
        "            noise = torch.randn_like(data_noisy.x[idx]) * 0.1  # Adjust noise level if needed\n",
        "            data_noisy.x[idx] += noise\n",
        "\n",
        "        noisy_data_list.append(data_noisy)\n",
        "\n",
        "    return noisy_data_list\n",
        "\n",
        "# ✅ Apply noise\n",
        "noisy_data_list = inject_noise(data_list, label_noise_ratio=0.1, feature_noise_ratio=0.1)\n",
        "# Add time_window attribute to each data object\n",
        "for t, data in enumerate(noisy_data_list):\n",
        "    data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "B0juN7K3MdOh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify GraphSAGE to Handle Causal Sampling:"
      ],
      "metadata": {
        "id": "d4sqBX7LMjfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CausalGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CausalGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        # Custom causal neighbor sampling by excluding edges based on time window\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Filter edges based on the time window (ensure causal sampling)\n",
        "        mask = time_window[row] <= time_window[col]\n",
        "        edge_index = edge_index[:, mask]\n",
        "\n",
        "        # Perform GraphSAGE forward pass using filtered edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gc2Px8ZFMiW5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop with Causal Training:"
      ],
      "metadata": {
        "id": "1KdpiDIDSuLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize the model\n",
        "model = CausalGraphSAGE(in_channels, hidden_channels, out_channels).to(\"cpu\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Causal training on noisy data\n",
        "model.train()\n",
        "for epoch in range(20):\n",
        "    total_loss = 0\n",
        "    for t in range(1, len(noisy_data_list)):\n",
        "        optimizer.zero_grad()\n",
        "        past_data = noisy_data_list[t - 1].to(\"cpu\")\n",
        "        current_data = noisy_data_list[t].to(\"cpu\")\n",
        "\n",
        "        # Pass time_window as an argument to the model\n",
        "        out_past = model(past_data.x, past_data.edge_index, past_data.time_window)\n",
        "        loss_past = loss_fn(out_past, past_data.y)\n",
        "\n",
        "        out_current = model(current_data.x, current_data.edge_index, current_data.time_window)\n",
        "        loss_current = loss_fn(out_current, current_data.y)\n",
        "\n",
        "        loss = loss_past + loss_current\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Noise] Epoch {epoch+1}, Causal Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4hpGJbgSvVr",
        "outputId": "6b540367-5684-4908-bc41-61f3fcb45446"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Noise] Epoch 1, Causal Loss: 0.0000\n",
            "[Noise] Epoch 2, Causal Loss: 0.0000\n",
            "[Noise] Epoch 3, Causal Loss: 0.0000\n",
            "[Noise] Epoch 4, Causal Loss: 0.0000\n",
            "[Noise] Epoch 5, Causal Loss: 0.0000\n",
            "[Noise] Epoch 6, Causal Loss: 0.0000\n",
            "[Noise] Epoch 7, Causal Loss: 0.0000\n",
            "[Noise] Epoch 8, Causal Loss: 0.0000\n",
            "[Noise] Epoch 9, Causal Loss: 0.0000\n",
            "[Noise] Epoch 10, Causal Loss: 0.0000\n",
            "[Noise] Epoch 11, Causal Loss: 0.0000\n",
            "[Noise] Epoch 12, Causal Loss: 0.0000\n",
            "[Noise] Epoch 13, Causal Loss: 0.0000\n",
            "[Noise] Epoch 14, Causal Loss: 0.0000\n",
            "[Noise] Epoch 15, Causal Loss: 0.0000\n",
            "[Noise] Epoch 16, Causal Loss: 0.0000\n",
            "[Noise] Epoch 17, Causal Loss: 0.0000\n",
            "[Noise] Epoch 18, Causal Loss: 0.0000\n",
            "[Noise] Epoch 19, Causal Loss: 0.0000\n",
            "[Noise] Epoch 20, Causal Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Verify:\n",
        "After you inject noise and assign the time_window to each data object:"
      ],
      "metadata": {
        "id": "MMpuE801UEa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add time_window attribute to each data object\n",
        "for t, data in enumerate(noisy_data_list):\n",
        "    data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)\n",
        "\n",
        "# Verify\n",
        "for data in noisy_data_list:\n",
        "    print(data.time_window)  # Should print a tensor with the same length as the number of nodes in each graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWyT3dACUF27",
        "outputId": "f53d71ba-4c6e-4416-e2e8-871eae93f3c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
            "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
            "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
            "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
            "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
            "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
            "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
            "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
            "tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16])\n",
            "tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18])\n",
            "tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19])\n",
            "tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20])\n",
            "tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21])\n",
            "tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22])\n",
            "tensor([23, 23, 23, 23, 23, 23, 23, 23, 23, 23])\n",
            "tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24])\n",
            "tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25])\n",
            "tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26])\n",
            "tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27])\n",
            "tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28])\n",
            "tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29])\n",
            "tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30])\n",
            "tensor([31, 31, 31, 31, 31, 31, 31, 31, 31, 31])\n",
            "tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32])\n",
            "tensor([33, 33, 33, 33, 33, 33, 33, 33, 33, 33])\n",
            "tensor([34, 34, 34, 34, 34, 34, 34, 34, 34, 34])\n",
            "tensor([35, 35, 35, 35, 35, 35, 35, 35, 35, 35])\n",
            "tensor([36, 36, 36, 36, 36, 36, 36, 36, 36, 36])\n",
            "tensor([37, 37, 37, 37, 37, 37, 37, 37, 37, 37])\n",
            "tensor([38, 38, 38, 38, 38, 38, 38, 38, 38, 38])\n",
            "tensor([39, 39, 39, 39, 39, 39, 39, 39, 39, 39])\n",
            "tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40])\n",
            "tensor([41, 41, 41, 41, 41, 41, 41, 41, 41, 41])\n",
            "tensor([42, 42, 42, 42, 42, 42, 42, 42, 42, 42])\n",
            "tensor([43, 43, 43, 43, 43, 43, 43, 43, 43, 43])\n",
            "tensor([44, 44, 44, 44, 44, 44, 44, 44, 44, 44])\n",
            "tensor([45, 45, 45, 45, 45, 45, 45, 45, 45, 45])\n",
            "tensor([46, 46, 46, 46, 46, 46, 46, 46, 46, 46])\n",
            "tensor([47, 47, 47, 47, 47, 47, 47, 47, 47, 47])\n",
            "tensor([48, 48, 48, 48, 48, 48, 48, 48, 48, 48])\n",
            "tensor([49, 49, 49, 49, 49, 49, 49, 49, 49, 49])\n",
            "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50])\n",
            "tensor([51, 51, 51, 51, 51, 51, 51, 51, 51, 51])\n",
            "tensor([52, 52, 52, 52, 52, 52, 52, 52, 52, 52])\n",
            "tensor([53, 53, 53, 53, 53, 53, 53, 53, 53, 53])\n",
            "tensor([54, 54, 54, 54, 54, 54, 54, 54, 54, 54])\n",
            "tensor([55, 55, 55, 55, 55, 55, 55, 55, 55, 55])\n",
            "tensor([56, 56, 56, 56, 56, 56, 56, 56, 56, 56])\n",
            "tensor([57, 57, 57, 57, 57, 57, 57, 57, 57, 57])\n",
            "tensor([58, 58, 58, 58, 58, 58, 58, 58, 58, 58])\n",
            "tensor([59, 59, 59, 59, 59, 59, 59, 59, 59, 59])\n",
            "tensor([60, 60, 60, 60, 60, 60, 60, 60, 60, 60])\n",
            "tensor([61, 61, 61, 61, 61, 61, 61, 61, 61, 61])\n",
            "tensor([62, 62, 62, 62, 62, 62, 62, 62, 62, 62])\n",
            "tensor([63, 63, 63, 63, 63, 63, 63, 63, 63, 63])\n",
            "tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64])\n",
            "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65, 65])\n",
            "tensor([66, 66, 66, 66, 66, 66, 66, 66, 66, 66])\n",
            "tensor([67, 67, 67, 67, 67, 67, 67, 67, 67, 67])\n",
            "tensor([68, 68, 68, 68, 68, 68, 68, 68, 68, 68])\n",
            "tensor([69, 69, 69, 69, 69, 69, 69, 69, 69, 69])\n",
            "tensor([70, 70, 70, 70, 70, 70, 70, 70, 70, 70])\n",
            "tensor([71, 71, 71, 71, 71, 71, 71, 71, 71, 71])\n",
            "tensor([72, 72, 72, 72, 72, 72, 72, 72, 72, 72])\n",
            "tensor([73, 73, 73, 73, 73, 73, 73, 73, 73, 73])\n",
            "tensor([74, 74, 74, 74, 74, 74, 74, 74, 74, 74])\n",
            "tensor([75, 75, 75, 75, 75, 75, 75, 75, 75, 75])\n",
            "tensor([76, 76, 76, 76, 76, 76, 76, 76, 76, 76])\n",
            "tensor([77, 77, 77, 77, 77, 77, 77, 77, 77, 77])\n",
            "tensor([78, 78, 78, 78, 78, 78, 78, 78, 78, 78])\n",
            "tensor([79, 79, 79, 79, 79, 79, 79, 79, 79, 79])\n",
            "tensor([80, 80, 80, 80, 80, 80, 80, 80, 80, 80])\n",
            "tensor([81, 81, 81, 81, 81, 81, 81, 81, 81, 81])\n",
            "tensor([82, 82, 82, 82, 82, 82, 82, 82, 82, 82])\n",
            "tensor([83, 83, 83, 83, 83, 83, 83, 83, 83, 83])\n",
            "tensor([84, 84, 84, 84, 84, 84, 84, 84, 84, 84])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with Strict vs. Non-Strict Causality:\n",
        "Non-Strict Causality (<=): Allows nodes to consider neighbors from the current window (i.e., nodes can influence each other within the same time window).\n",
        "\n",
        "Strict Causality (<): Prevents nodes from considering neighbors from the same window (i.e., nodes can only influence past time windows)."
      ],
      "metadata": {
        "id": "X7bRFbq6UQmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Code for Training Loop with Causal Sampling:"
      ],
      "metadata": {
        "id": "71srxivCVp9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "The time_window should be an attribute of each Data object.\n",
        "\n",
        "When performing causal sampling during the training or forward pass, reference the time_window correctly using the data.time_window[row] format.\n",
        "\n",
        "Correctly apply the mask based on your causality preference (<= for non-strict, < for strict)."
      ],
      "metadata": {
        "id": "-0uqDGSnVwL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming this is inside your model's forward pass or during training\n",
        "for t, data in enumerate(noisy_data_list):\n",
        "    data.time_window = torch.full((data.num_nodes,), t, dtype=torch.long)  # Set time_window if not already done\n",
        "\n",
        "    # Your GraphSAGE processing here (with causal sampling logic)\n",
        "    for row, col in zip(data.edge_index[0], data.edge_index[1]):\n",
        "        # Apply causal mask depending on the choice of causality (strict vs non-strict)\n",
        "        mask = data.time_window[row] <= data.time_window[col]  # Non-strict\n",
        "        # mask = data.time_window[row] < data.time_window[col]  # Strict\n",
        "\n",
        "        # Check if the mask for this edge is True\n",
        "        if mask.item():  # Use .item() to get the boolean value from the tensor\n",
        "            # Proceed with aggregation for valid neighbors\n",
        "            # For example, aggregate features here for GraphSAGE\n",
        "            pass\n"
      ],
      "metadata": {
        "id": "IqZgQk9gUbk-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aggregation of Valid Neighbors:\n",
        "We will aggregate features from valid neighbors based on the causal mask. For simplicity, let's assume you're summing the features of valid neighbors. If you want to experiment with other aggregation methods like average or max, you can easily modify this logic.\n",
        "\n",
        "2. Update Node Representations:\n",
        "After aggregating the valid neighbors, we combine the node's features with the aggregated features (as typically done in GraphSAGE).\n",
        "\n",
        "3. Complete Forward Pass:\n",
        "The forward pass will involve processing each graph in noisy_data_list, applying the aggregation logic, and passing the updated node representations through the model.\n",
        "\n",
        "4. Training Loop:\n",
        "We’ll set up the training loop using your loss function (CrossEntropyLoss) and optimizer (Adam). We'll update the model's weights and compute the loss after each batch.\n",
        "\n",
        "5. Evaluation:\n",
        "After training, we’ll evaluate the model on a test set, and you can experiment with different metrics based on your task."
      ],
      "metadata": {
        "id": "5x88qzAEWSuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of features in the first window: {noisy_data_list[0].x.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA-p4IpyZLvl",
        "outputId": "6dd5383a-5a3d-4e7f-f492-8139168f49f9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features in the first window: torch.Size([10, 55])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(2 * in_channels, hidden_channels, kernel_size=1)\n",
        "        self.conv2 = torch.nn.Conv1d(hidden_channels, out_channels, kernel_size=1)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "    def forward(self, x, edge_index, time_window):\n",
        "        x = x.unsqueeze(0).transpose(1, 2)\n",
        "        aggregated_features = torch.zeros_like(x)\n",
        "        for row, col in zip(edge_index[0], edge_index[1]):\n",
        "            mask = time_window[row] <= time_window[col]\n",
        "            if mask.item():\n",
        "                aggregated_features[0, :, row] += x[0, :, col]\n",
        "        aggregated_features = aggregated_features / (aggregated_features.sum(dim=2, keepdim=True) + 1e-8)\n",
        "        combined_features = torch.cat([x, aggregated_features], dim=1)\n",
        "        out = F.relu(self.conv1(combined_features))\n",
        "        out = self.conv2(out)\n",
        "        out = out.squeeze(0).transpose(0, 1)\n",
        "        return out\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.time_window)\n",
        "            loss = loss_fn(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            out = model(data.x, data.edge_index, data.time_window)\n",
        "            _, predicted = torch.max(out, dim=1)\n",
        "            total += data.y.size(0)\n",
        "            correct += (predicted == data.y).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "# Define hyperparameters\n",
        "in_channels = 55\n",
        "hidden_channels = 32\n",
        "out_channels = 2\n",
        "learning_rate = 0.01\n",
        "batch_size = 1 # Process each window individually for simplicity\n",
        "\n",
        "# Assuming you have your noisy_data_list\n",
        "all_data = noisy_data_list\n",
        "\n",
        "# Split the data into training and validation sets (80% train, 20% validation)\n",
        "train_data_list, val_data_list = train_test_split(all_data, test_size=0.2, random_state=42, stratify=[data.y.cpu().numpy() for data in all_data])\n",
        "\n",
        "print(f\"Number of training samples: {len(train_data_list)}\")\n",
        "print(f\"Number of validation samples: {len(val_data_list)}\")\n",
        "\n",
        "# Create data loaders (even if batch size is 1 for now)\n",
        "train_loader = train_data_list\n",
        "val_loader = val_data_list\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = GraphSAGE(in_channels, hidden_channels, out_channels)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model on the training set\n",
        "train(model, optimizer, loss_fn, train_loader, num_epochs=20)\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Malware/malware_model.pth')\n",
        "print(\"Trained model saved to 'malware_model.pth'\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "evaluate(model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j7ju8gkWTNo",
        "outputId": "1158df1e-cb54-4e12-9f1d-4937032d5eb7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 68\n",
            "Number of validation samples: 17\n",
            "Epoch 1, Loss: 2.0867\n",
            "Epoch 2, Loss: 0.0034\n",
            "Epoch 3, Loss: 0.0013\n",
            "Epoch 4, Loss: 0.0010\n",
            "Epoch 5, Loss: 0.0008\n",
            "Epoch 6, Loss: 0.0007\n",
            "Epoch 7, Loss: 0.0006\n",
            "Epoch 8, Loss: 0.0005\n",
            "Epoch 9, Loss: 0.0004\n",
            "Epoch 10, Loss: 0.0004\n",
            "Epoch 11, Loss: 0.0003\n",
            "Epoch 12, Loss: 0.0003\n",
            "Epoch 13, Loss: 0.0002\n",
            "Epoch 14, Loss: 0.0002\n",
            "Epoch 15, Loss: 0.0002\n",
            "Epoch 16, Loss: 0.0002\n",
            "Epoch 17, Loss: 0.0002\n",
            "Epoch 18, Loss: 0.0001\n",
            "Epoch 19, Loss: 0.0001\n",
            "Epoch 20, Loss: 0.0001\n",
            "Trained model saved to 'malware_model.pth'\n",
            "Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Out-of-Distribution (OOD) Testing\n",
        "\n",
        "---The approach here will be to treat the 80% training data as your \"in-distribution\" data and the 20% validation data as a limited proxy for \"out-of-distribution\" data. However, it's crucial to remember the limitations: this is not true OOD as it's still from the same original dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zf4yNl88bxb9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}